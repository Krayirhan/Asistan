"""
Benchmark - Performans testleri
RTX 2060 Super iÃ§in beklenen sÃ¼reler
"""

import time
from src.core.model_loader import ModelManager
from src.core.llm_manager import LLMManager
from src.monitoring.performance import PerformanceTracker


def benchmark_llm_inference():
    """LLM inference hÄ±zÄ±"""
    
    config = {
        'hardware': {'gpu_memory_limit': 7.5, 'model_unload_timeout': 30},
        'llm': {
            'model': 'qwen2.5:3b-instruct-q4_K_M',
            'max_tokens': 100,
            'temperature': 0.7,
            'stream': False
        },
        'memory': {'max_history': 15, 'save_to_disk': False}
    }
    
    print("\n" + "="*60)
    print("LLM INFERENCE BENCHMARK")
    print("="*60)
    
    model_manager = ModelManager(config)
    llm_manager = LLMManager(config, model_manager)
    tracker = PerformanceTracker()
    
    queries = [
        "Merhaba, nasÄ±lsÄ±n?",
        "Python nedir?",
        "2+2 kaÃ§ eder?",
        "Yapay zeka nedir?",
        "BugÃ¼n hava nasÄ±l?"
    ]
    
    for i, query in enumerate(queries, 1):
        print(f"\n[{i}/5] Soru: {query}")
        
        tracker.start_operation('llm_inference')
        response = llm_manager.generate(query, stream=False)
        tracker.end_operation('llm_inference')
        
        print(f"Cevap: {response[:60]}...")
    
    # Ä°statistikler
    stats = tracker.get_statistics('llm_inference')
    
    print("\n" + "="*60)
    print("SONUÃ‡LAR:")
    print("="*60)
    print(f"Toplam Sorgu: {stats['count']}")
    print(f"Toplam SÃ¼re: {stats['total_time']}s")
    print(f"Ortalama SÃ¼re: {stats['average_time']}s")
    print(f"Min SÃ¼re: {stats['min_time']}s")
    print(f"Max SÃ¼re: {stats['max_time']}s")
    
    # RTX 2060 Super iÃ§in beklenen: 3-5s
    print("\nğŸ“Š Beklenen Performans (RTX 2060 Super):")
    print("   Ortalama: 3-5 saniye")
    
    if stats['average_time'] < 5:
        print("   âœ… Performans iyi!")
    elif stats['average_time'] < 7:
        print("   âš ï¸  Performans kabul edilebilir")
    else:
        print("   âŒ Performans dÃ¼ÅŸÃ¼k, optimizasyon gerekli")


def benchmark_model_loading():
    """Model yÃ¼kleme sÃ¼releri"""
    
    config = {
        'hardware': {'gpu_memory_limit': 7.5, 'model_unload_timeout': 30},
        'llm': {'model': 'qwen2.5:3b-instruct-q4_K_M'},
        'vlm': {'model': 'moondream'},
        'stt': {'model_size': 'medium', 'device': 'cuda', 'compute_type': 'int8'}
    }
    
    print("\n" + "="*60)
    print("MODEL LOADING BENCHMARK")
    print("="*60)
    
    model_manager = ModelManager(config)
    tracker = PerformanceTracker()
    
    models = ['llm', 'vlm', 'stt']
    
    for model_name in models:
        print(f"\n[TEST] {model_name.upper()} yÃ¼kleniyor...")
        
        tracker.start_operation(f'{model_name}_load')
        try:
            model_manager.load_model(model_name)
            tracker.end_operation(f'{model_name}_load')
            
            vram = model_manager.get_vram_usage()
            print(f"VRAM: {vram:.2f}GB")
            
        except Exception as e:
            print(f"Hata: {e}")
    
    # Ä°statistikler
    print("\n" + "="*60)
    print("YÃœKLEME SÃœRELERÄ°:")
    print("="*60)
    
    for model_name in models:
        stats = tracker.get_statistics(f'{model_name}_load')
        if stats['count'] > 0:
            print(f"{model_name.upper()}: {stats['average_time']}s")


if __name__ == "__main__":
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                                                           â•‘
    â•‘        PERFORMANCE BENCHMARK                             â•‘
    â•‘        RTX 2060 Super (8GB VRAM)                         â•‘
    â•‘                                                           â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    try:
        benchmark_model_loading()
    except Exception as e:
        print(f"\nâŒ Model loading benchmark hatasÄ±: {e}")
    
    try:
        benchmark_llm_inference()
    except Exception as e:
        print(f"\nâŒ LLM inference benchmark hatasÄ±: {e}")
    
    print("\n" + "="*60)
    print("BENCHMARK TAMAMLANDI")
    print("="*60)
